{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SsWH_IvBEsWv",
        "XLNPfGtdh_Hk",
        "Cf-zNI2dstHy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EarlLem/462-GAN/blob/main/Neural_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiLQ8YUgElTB"
      },
      "source": [
        "# Первая рабочая версия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsWH_IvBEsWv"
      },
      "source": [
        "## Импорты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMRbWAGBHjSO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from scipy.stats import binom, uniform\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZNiBG-XEvBp"
      },
      "source": [
        "## Генератор ридов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NU9yr_HExyf"
      },
      "source": [
        "def get_rand_ind(left: int, right: int, count: int) -> list:\n",
        "    res_indexes = set()\n",
        "    while len(res_indexes) < count:\n",
        "        new_index = random.randrange(left, right)\n",
        "        if new_index not in res_indexes:\n",
        "            res_indexes.add(new_index)\n",
        "    return list(res_indexes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmRjTKNRE2Xd"
      },
      "source": [
        "## Генерация датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORnetH4bE6S4"
      },
      "source": [
        "# фиксированная длина ридов и фиксированная ошибка\n",
        "read_length = 60\n",
        "#интервал ошибок\n",
        "a, b = (0.001, 0.05)\n",
        "#размер тренировочного набора и тестового\n",
        "train_size = 1000\n",
        "test_size = 500\n",
        "#количество ридов\n",
        "N_reads = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnw5QyhmH_lW"
      },
      "source": [
        "train_y = np.random.uniform(low=0.0, high=1.0, size=train_size)\n",
        "train_y = train_y.round().astype('int32')\n",
        "test_y = np.random.uniform(low=0.0, high=1.0, size=test_size)\n",
        "test_y = test_y.round().astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWhuhHW2Fh8V"
      },
      "source": [
        "def dataset_generator(dataset_identifier):\n",
        "  dataset = np.empty((dataset_identifier.shape[0], N_reads, read_length))\n",
        "  sum_rl = 0\n",
        "  for i in range(1, read_length+1):\n",
        "    sum_rl += 1 / i\n",
        "  for _i in range(dataset_identifier.shape[0]):\n",
        "    data = []\n",
        "    err = np.exp(random.uniform(np.log(a), np.log(b)))\n",
        "    corr = read_length * err / sum_rl\n",
        "    if dataset_identifier[_i] == 0:\n",
        "      for _j in range(N_reads):\n",
        "        k = binom.rvs(read_length, err)\n",
        "        err_lst = get_rand_ind(0, read_length, k)\n",
        "        res = [1 if i in err_lst else 0 for i in range(read_length)]\n",
        "        data.append(res)\n",
        "    elif dataset_identifier[_i] == 1:\n",
        "      for _j in range(N_reads):\n",
        "        cube = [random.uniform(0, 1) for i in range(read_length)]\n",
        "        p = [1/x for x in range(1, read_length+1)]\n",
        "        res = [1 if cube[i] < p[i] * corr else 0 for i in range(read_length)]\n",
        "        data.append(res)\n",
        "    data = np.array(data)\n",
        "    dataset[_i] = data\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29wlYD82FiNC"
      },
      "source": [
        "train_x = dataset_generator(train_y)\n",
        "test_x = dataset_generator(test_y)\n",
        "#тесил что форма совпадает с параметрами(работает долго, сначала советую позапускать при малых N_reads)\n",
        "print(train_x.shape, train_x[0].shape, train_x[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiW6E9ghH-W"
      },
      "source": [
        "#Разбиение массива тренироваочных данных на блоки\n",
        "BATCH_SIZE = 50\n",
        "reads_tensor = tf.convert_to_tensor(train_x)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(reads_tensor).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edrJUUezGlD1"
      },
      "source": [
        "## Инициализация и обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfRnEKuYGkrP"
      },
      "source": [
        "#Находит среднее от каждой строки тензора\n",
        "def meanlayer(tensors):\n",
        "  out = tf.reduce_mean(tensors, axis=1)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIgIjaI9G5XB"
      },
      "source": [
        "model = Sequential()\n",
        "#Сверточный слой выделяющий вектор признаков\n",
        "model.add(tf.keras.layers.Conv1D(read_length/4, 1, activation='relu', input_shape=(N_reads, read_length)))\n",
        "#Слой, подсчитывающий среднее каждого вектора признаков\n",
        "model.add(tf.keras.layers.Lambda(meanlayer))\n",
        "#Плотные скрытые слои по read_length нейронов\n",
        "model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "#Выходной слой(ошибки 2 - поэтому размерность 2)\n",
        "model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n",
        "#Сборка\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#Обучение\n",
        "model.fit(train_x, train_y, epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIH3A3IjHshH"
      },
      "source": [
        "#Проверка на тестовом датасете\n",
        "val_loss, val_acc = model.evaluate(test_x, test_y)\n",
        "print(val_loss, val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmagT8KX5nq"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(N_reads, read_length)),\n",
        "    keras.layers.Dense(45, activation=tf.nn.relu),\n",
        "\tkeras.layers.Dense(20, activation=tf.nn.relu),\n",
        "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_x, train_y, epochs=50)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxu3v80V-VKN"
      },
      "source": [
        "# GAN try 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7lJE-Z4YTqk"
      },
      "source": [
        "train_y = np.ones(train_size)\n",
        "train_x = dataset_generator(train_y)\n",
        "print(train_x.shape, train_x[0].shape, train_x[0][0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33hx-8KS-l5Y"
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "reads_tensor = tf.convert_to_tensor(train_x)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(reads_tensor).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyE4qRvP-pBR"
      },
      "source": [
        "def make_discriminator_model():\n",
        "  model = Sequential()\n",
        "  #Сверточный слой выделяющий вектор признаков\n",
        "  model.add(tf.keras.layers.Conv1D(15, 1, activation='relu', input_shape=(N_reads, read_length)))\n",
        "  #Слой, подсчитывающий среднее каждого вектора признаков\n",
        "  model.add(tf.keras.layers.Lambda(meanlayer))\n",
        "  #Плотные скрытые слои по read_length нейронов\n",
        "  model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "  #Выходной слой\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI9qJgr--6l_"
      },
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmRs1QnN-xHN"
      },
      "source": [
        "def make_generator_model():\n",
        "  model = Sequential()\n",
        "  model.add(tf.keras.layers.Dense(N_reads, use_bias=False, input_shape=(N_reads, )))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(tf.keras.layers.Reshape((1000,1)))\n",
        "  model.add(tf.keras.layers.Conv1DTranspose(12, 1, padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Conv1DTranspose(60, 1, padding=\"same\", activation=\"sigmoid\"))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPAC981i-1hx"
      },
      "source": [
        "generator = make_generator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4B6VQuf--hZ"
      },
      "source": [
        "cross_entropy = tf.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS2U5fCM_DfR"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UMGxL7S_JNs"
      },
      "source": [
        "import time\n",
        "EPOCHS = 50\n",
        "noise_dim = N_reads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYY8W5D3_MdC"
      },
      "source": [
        "def train_step(matrices):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_matrices = generator(noise, training=True)\n",
        "      histogram(generated_matrices)\n",
        "\n",
        "      real_output = discriminator(matrices, training=True)\n",
        "      fake_output = discriminator(generated_matrices, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    print(\"generator loss: \", np.mean(gen_loss))\n",
        "    print(\"discriminator loss: \", np.mean(disc_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SvWJdxD_QId"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for matrix_batch in dataset:\n",
        "      train_step(matrix_batch)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mML5uD7I_TlE"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLNPfGtdh_Hk"
      },
      "source": [
        "# GAN try Тая"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YTC427Xi4lv"
      },
      "source": [
        "def dataset_generator(dataset_identifier):\n",
        "  dataset = np.empty((dataset_identifier.shape[0], N_reads, read_length))\n",
        "  sum_rl = 0\n",
        "  for i in range(1, read_length+1):\n",
        "    sum_rl += 1 / i\n",
        "  for _i in range(dataset_identifier.shape[0]):\n",
        "    data = []\n",
        "    err = np.exp(random.uniform(np.log(a), np.log(b)))\n",
        "    corr = read_length * err / sum_rl\n",
        "    if dataset_identifier[_i] == 0:\n",
        "      for _j in range(N_reads):\n",
        "        k = binom.rvs(read_length, err)\n",
        "        err_lst = get_rand_ind(0, read_length, k)\n",
        "        res = [1 if i in err_lst else 0 for i in range(read_length)]\n",
        "        data.append(res)\n",
        "    elif dataset_identifier[_i] == 1:\n",
        "      for _j in range(N_reads):\n",
        "        cube = [random.uniform(0, 1) for i in range(read_length)]\n",
        "        p = [1/x for x in range(1, read_length+1)]\n",
        "        res = [1 if cube[i] < p[i] * corr else 0.1 for i in range(read_length)]\n",
        "        data.append(res)\n",
        "    data = np.array(data)\n",
        "    dataset[_i] = data\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSPnZo1Sko2w"
      },
      "source": [
        "def get_rand_ind(left: int, right: int, count: int) -> list:\n",
        "    res_indexes = set()\n",
        "    while len(res_indexes) < count:\n",
        "        new_index = random.randrange(left, right)\n",
        "        if new_index not in res_indexes:\n",
        "            res_indexes.add(new_index)\n",
        "    return list(res_indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9UECUyXkoBf"
      },
      "source": [
        "#Находит среднее от каждой строки тензора\n",
        "def meanlayer(tensors):\n",
        "  out = tf.reduce_mean(tensors, axis=1)\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeA0aX7CiO_V"
      },
      "source": [
        "# фиксированная длина ридов и фиксированная ошибка\n",
        "read_length = 60\n",
        "#интервал ошибок\n",
        "a, b = (0.001, 0.05)\n",
        "#размер тренировочного набора и тестового\n",
        "train_size = 1000\n",
        "test_size = 50\n",
        "#количество ридов\n",
        "N_reads = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRXG4Oipj-SE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def histogram(data, size):\n",
        "  X = np.arange(read_length)\n",
        "  # Y = np.sum(data, axis=(0, 1)) / (N_reads*size)\n",
        "  Y = np.sum(data, axis=(1, 0))\n",
        "  plt.plot(X, Y, \"o\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KutFYVkoieSP"
      },
      "source": [
        "# y_train = np.random.uniform(low=0.6, high=1, size=train_size)\n",
        "# y_train = y_train.round().astype('int32')\n",
        "y_train = np.ones(train_size)\n",
        "X_train = dataset_generator(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQViE7WylGAV"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNCAwZyOhNHv"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv7DLXwij7If"
      },
      "source": [
        "histogram(X_train, train_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnzJ6VLHirBS"
      },
      "source": [
        "batch_size = 100\n",
        "epoches = 200\n",
        "const = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKYA6QAsi73U"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, Conv1DTranspose, Conv1D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "\n",
        "\n",
        "def generator_model():\n",
        "    model = Sequential()\n",
        "    # model.add(Conv1D(read_length, kernel_size=3, padding='same'))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(Dense(int(read_length / const)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv1DTranspose(read_length, kernel_size=3, padding='same', input_shape=(N_reads, int(read_length / const))))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def discriminator_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv1D(15, 3, activation='relu'))\n",
        "    model.add(Lambda(meanlayer))\n",
        "    model.add(Dense(read_length, activation='relu'))\n",
        "    model.add(Dense(read_length, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model    \n",
        "\n",
        "def generator_containing_discriminator(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    discriminator.trainable = False\n",
        "    model.add(discriminator)\n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cDdcAkWjCQQ"
      },
      "source": [
        "def train(BATCH_SIZE):\n",
        "\n",
        "    discriminator = discriminator_model()\n",
        "    generator = generator_model()\n",
        "    discriminator_on_generator = \\\n",
        "        generator_containing_discriminator(generator, discriminator)\n",
        "    # d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    d_optim = tf.keras.optimizers.Adam(lr=0.0003, beta_1=0.5)\n",
        "    g_optim = tf.keras.optimizers.Adam(lr=0.0003, beta_1=0.5)\n",
        "    # g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
        "    generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    discriminator_on_generator.compile(\n",
        "        loss='binary_crossentropy', optimizer=g_optim)\n",
        "    discriminator.trainable = True\n",
        "    discriminator.compile(loss='sparse_categorical_crossentropy', optimizer=d_optim)\n",
        "    # noise = np.zeros((BATCH_SIZE, 100))\n",
        "    for epoch in range(epoches):\n",
        "        print(\"Epoch is\", epoch)\n",
        "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
        "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
        "            noise = np.array(np.random.uniform(0, 1, size=(BATCH_SIZE, N_reads, int(read_length / const ))))\n",
        "            # noise = np.array(np.random.uniform(0, 1, size=(BATCH_SIZE, N_reads, read_length)))\n",
        "            \n",
        "            data_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "            # print(data_batch.shape)\n",
        "            generated_data = generator.predict(noise, verbose=0)\n",
        "            histogram(generated_data, BATCH_SIZE)#graph\n",
        "            X = np.concatenate((data_batch, generated_data))\n",
        "            y = np.array([1] * BATCH_SIZE + [0] * BATCH_SIZE).reshape(2*BATCH_SIZE, 1)\n",
        "            d_loss = discriminator.train_on_batch(X, y)\n",
        "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
        "            noise = np.array(np.random.uniform(0, 1, size=(batch_size, N_reads, int(read_length / const))))\n",
        "            # noise = np.array(np.random.uniform(0, 1, size=(BATCH_SIZE, N_reads, read_length)))    \n",
        "            discriminator.trainable = False\n",
        "            g_loss = discriminator_on_generator.train_on_batch(noise, np.array([1] * BATCH_SIZE))\n",
        "            discriminator.trainable = True\n",
        "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
        "            if index % 10 == 9:\n",
        "                generator.save_weights('generator', True)\n",
        "                discriminator.save_weights('discriminator', True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97KHKZumjnV9"
      },
      "source": [
        "train(BATCH_SIZE=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf-zNI2dstHy"
      },
      "source": [
        "# Ган by Лидия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dXn2648Y6gl"
      },
      "source": [
        "def define_generator():\n",
        "  model = Sequential()\n",
        "  model.add(tf.keras.layers.Conv1D(15, 1, activation='relu', input_shape=(N_reads, read_length)))\n",
        "  model.add(tf.keras.layers.Lambda(meanlayer, (N_reads, 1)))\n",
        "  model.add(tf.keras.layers.Dense(2))\n",
        "  model.add(tf.keras.layers.Dense(read_length, activation = 'relu'))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
        "  model.add(tf.keras.layers.Dense(read_length/4))\n",
        "  #model.add(tf.keras.layers.Dense(4, activation='sigmoid'))\n",
        "  model.add(tf.keras.layers.Dense(np.prod(train_x[0].shape), activation='tanh'))\n",
        "  model.add(tf.keras.layers.Reshape(train_x[0].shape))\n",
        "  return model\n",
        "\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(g_model)\n",
        "\t# add the discriminator\n",
        "\tmodel.add(d_model)\n",
        "\t# compile model\n",
        "\topt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n):\n",
        "  # generate points in the latent space\n",
        "  x_input = np.random.randint(60, size = latent_dim * n_samples*n)\n",
        "  # reshape into a batch of inputs for the network\n",
        "  x_input = x_input.reshape(n_samples, n, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples, n):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples, n)\n",
        "\t# predict outputs\n",
        "\tX = g_model.predict(x_input)\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = np.zeros((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "def histogram(data):\n",
        "  X = np.arange(1, read_length + 1)\n",
        "  Y = np.sum(data, axis=(0, 1))\n",
        "  plt.plot(X, Y, \"o\")\n",
        "  plt.show()\n",
        "\n",
        "def summarize_performance(epoch, g_model, d_model, X, Y):\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X, Y, verbose=0)\n",
        "\t# prepare fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, read_length, N_reads, N_reads)\n",
        "\t# evaluate discriminator on fake examples\n",
        "  #x_fake = tf.math.round(x_fake)\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\t# summarize discriminator performance\n",
        "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\t# save the generator model tile file\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(filename)\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, train_x, train_y, n_epochs=100, n_batch=50):\n",
        "  bat_per_epo = int(train_x.shape[0] / n_batch)\n",
        "  ax = np.arange(1, 60, 1)\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "  # enumerate batches over the training set\n",
        "    for j in range(bat_per_epo):\n",
        "      d_loss1, _ = d_model.train_on_batch(train_x, train_y)\n",
        "      # generate 'fake' examples\n",
        "      X_fake, y_fake = generate_fake_samples(g_model, read_length, N_reads, N_reads)\n",
        "      # update discriminator model weights\n",
        "      X_fake = tf.math.round(X_fake)\n",
        "      d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "      # prepare points in latent space as input for the generator\n",
        "      X_gan = generate_latent_points(read_length, N_reads, N_reads)\n",
        "      # create inverted labels for the fake samples\n",
        "      y_gan = np.ones((N_reads, 1))\n",
        "      # update the generator via the discriminator's error\n",
        "      g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "      # summarize loss on this batch\n",
        "      #print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "      #(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "      # evaluate the model performance, sometimes\n",
        "    if (i+1) % 10 == 0:\n",
        "      summarize_performance(i, g_model, d_model, train_x, train_y)\n",
        "      histogram(X_fake)\n",
        "\n",
        "def define_discriminator():\n",
        "  model = Sequential()\n",
        "  #Сверточный слой выделяющий вектор признаков\n",
        "  model.add(tf.keras.layers.Conv1D(15, 1, activation='relu', input_shape=(N_reads, read_length)))\n",
        "  #Слой, подсчитывающий среднее каждого вектора признаков\n",
        "  model.add(tf.keras.layers.Lambda(meanlayer))\n",
        "  #Плотные скрытые слои по read_length нейронов\n",
        "  model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(read_length, activation='relu'))\n",
        "  #Выходной слой\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "#Сборка\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2AGtMnAtT1f"
      },
      "source": [
        "d_model = define_discriminator()\n",
        "g_model = define_generator()\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, descriminator)\n",
        "# train model\n",
        "train(g_model, d_model, gan_model, train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}